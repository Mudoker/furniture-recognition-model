{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "### COSC2753 - Machine Learning\n",
    "\n",
    "# **Model Development - Convolutional Neural Network (CNN)**\n",
    "\n",
    "<center>────────────────────────────</center>\n",
    "&nbsp;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Introduction\n",
    "\n",
    "In this notebook, we will focus on the development of a Convolutional Neural Network (CNN) model. This process will involve training the CNN model on preprocessed image data and optimizing its performance through hyperparameter tuning. Specifically, we will perform the following steps:\n",
    "\n",
    "- **Training:** We will train the selected CNN model using the preprocessed image data. This involves feeding the data into the model and adjusting its parameters to minimize the loss function.\n",
    "\n",
    "- **Hyperparameter Tuning:** We will explore different combinations of hyperparameters to optimize the performance of our CNN model. This may include tuning parameters such as learning rate, batch size, and regularization strength.\n",
    "\n",
    "- **Model Evaluation:** After training and tuning the CNN model, we will evaluate its performance using appropriate evaluation metrics. This step will help us assess how well the model generalizes to unseen data and determine its effectiveness in predicting labels for new images in the dataset.\n",
    "\n",
    "By the end of this notebook, we will have developed a well-trained CNN model and evaluated its performance, providing insights into its effectiveness for image recognition tasks. This model will serve as a foundation for further analysis and applications in image classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # OS related functions\n",
    "import numpy as np  # Numerical functions\n",
    "import pandas as pd  # Data manipulation\n",
    "import matplotlib.pyplot as plt  # Plotting\n",
    "import seaborn as sns  # Plotting\n",
    "\n",
    "# Deep learning\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential  # Pipeline\n",
    "from keras.layers import (\n",
    "    Dense,\n",
    "    Conv2D,\n",
    "    Flatten,\n",
    "    MaxPooling2D,\n",
    "    BatchNormalization,\n",
    "    Dropout,\n",
    ")  # Layers\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau  # Callbacks\n",
    "from tensorflow.keras.preprocessing.image import (\n",
    "    ImageDataGenerator,\n",
    ")  # Image data generator\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.metrics import classification_report  # Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link to support to the choice of: (Will be used later)\n",
    "\n",
    "batch size: https://medium.com/data-science-365/determining-the-right-batch-size-for-a-neural-network-to-get-better-and-faster-results-7a8662830f15#:~:text=It%20is%20a%20good%20practice,requires%20fewer%20epochs%20to%20converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"../../data/train/\") # Load train data\n",
    "df_test = pd.read_csv(\"../../data/test/\") # Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32  # Number of samples per gradient update\n",
    "num_classes = df_train[\"Category\"].nunique()  # Number of classes\n",
    "\n",
    "# Image data generator\n",
    "datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "\n",
    "# Common arguments\n",
    "common_args = {\n",
    "    \"x_col\": \"Path\",  # Path to image\n",
    "    \"y_col\": \"Category\",  # Target column\n",
    "    \"batch_size\": batch_size,  # Batch size\n",
    "    \"class_mode\": \"categorical\",  # Multi-class classification\n",
    "}\n",
    "\n",
    "# Create generator for training data\n",
    "train_dataset = datagen.flow_from_dataframe(\n",
    "    dataframe=df_train,  # Training data\n",
    "    shuffle=True,  # Shuffle the data\n",
    "    **common_args  # Common arguments\n",
    ")\n",
    "\n",
    "# Create generator for testing data\n",
    "test_dataset = datagen.flow_from_dataframe(\n",
    "    dataframe=df_test,  # Testing data\n",
    "    shuffle=False,  # Do not shuffle the data\n",
    "    **common_args  # Common arguments\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Model Development (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model Architecture Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link to support to the choice of: (Will be used later)\n",
    "\n",
    "- Convolutional Layers: https://www.linkedin.com/pulse/choosing-number-hidden-layers-neurons-neural-networks-sachdev/\n",
    "\n",
    "- Pooling Layers: https://machinelearningmastery.com/pooling-layers-for-convolutional-neural-networks/\n",
    "\n",
    "- Dropout Layers: https://nchlis.github.io/2017_08_10/page.html\n",
    "\n",
    "- Activation function choice: https://thangasami.medium.com/cnn-and-ann-performance-with-different-activation-functions-like-relu-selu-elu-sigmoid-gelu-etc-c542dd3b1365\n",
    "\n",
    "- Kernel size: https://medium.com/analytics-vidhya/significance-of-kernel-size-200d769aecb1#:~:text=Limiting%20the%20number%20of%20parameters,size%20at%203x3%20or%205x5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = Sequential()  # Pipeline\n",
    "\n",
    "# Convolutional layer 1\n",
    "cnn.add(Conv2D(32, kernel_size=(3, 3), activation=\"relu\"))  # Convolutional layer\n",
    "cnn.add(BatchNormalization())  # Batch normalization\n",
    "cnn.add(MaxPooling2D(pool_size=(2, 2)))  # Max pooling\n",
    "cnn.add(Dropout(0.2))  # Dropout\n",
    "\n",
    "# # Convolutional layer 2\n",
    "# cnn.add(Conv2D(64, kernel_size=(3, 3), activation=\"relu\"))  # Convolutional layer\n",
    "# cnn.add(BatchNormalization())  # Batch normalization\n",
    "# cnn.add(MaxPooling2D(pool_size=(2, 2)))  # Max pooling\n",
    "# cnn.add(Dropout(0.2))  # Dropout\n",
    "\n",
    "# # Convolutional layer 3\n",
    "# cnn.add(Conv2D(128, kernel_size=(3, 3), activation=\"relu\"))  # Convolutional layer\n",
    "# cnn.add(BatchNormalization())  # Batch normalization\n",
    "# cnn.add(MaxPooling2D(pool_size=(2, 2)))  # Max pooling\n",
    "# cnn.add(Dropout(0.2))  # Dropout\n",
    "\n",
    "# # Convolutional layer 4\n",
    "# cnn.add(Conv2D(256, kernel_size=(3, 3), activation=\"relu\"))  # Convolutional layer\n",
    "# cnn.add(BatchNormalization())  # Batch normalization\n",
    "# cnn.add(MaxPooling2D(pool_size=(2, 2)))  # Max pooling\n",
    "# cnn.add(Dropout(0.2))  # Dropout\n",
    "\n",
    "cnn.add(Flatten())  # Flatten\n",
    "\n",
    "cnn.add(Dense(512, activation=\"relu\"))  # Dense layer\n",
    "cnn.add(Dropout(0.5))  # Dropout\n",
    "# cnn.add(Dense(256, activation=\"relu\"))  # Dense layer\n",
    "# cnn.add(Dropout(0.5))  # Dropout\n",
    "\n",
    "cnn.add(Dense(num_classes, activation=\"softmax\"))  # Output layer\n",
    "\n",
    "cnn.summary()  # Model summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\",  # Monitor validation loss\n",
    "    patience=5,  # Stop training if no improvement for 5 epochs\n",
    "    restore_best_weights=True,  # Restore the best weights when stopping\n",
    "    min_delta=0.001,  # Minimum change to qualify as an improvement\n",
    "    verbose=1,  # Print messages\n",
    ")\n",
    "\n",
    "# Reduce learning rate\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",  # Monitor validation loss\n",
    "    patience=3,  # Reduce learning rate if no improvement for 3 epochs\n",
    "    factor=0.2,  # Reduce learning rate by a factor of 0.2\n",
    "    min_lr=0.00001,  # Minimum learning rate\n",
    "    verbose=1,  # Print messages\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(\n",
    "    optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")  # Compile model\n",
    "\n",
    "cnn.fit(\n",
    "    train_dataset,\n",
    "    validation_data=test_dataset,\n",
    "    epochs=10,  # 10 epochs\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    ")  # Fit model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "loss, accuracy = cnn.evaluate(test_dataset, verbose=0)\n",
    "\n",
    "print(f\"Loss: {loss:.2f}\")\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Predictions\n",
    "y_pred = cnn.predict(test_dataset)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# True values\n",
    "y_true = test_dataset.classes\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(y_true, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
